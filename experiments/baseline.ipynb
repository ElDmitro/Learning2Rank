{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import pickle\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_path = os.path.abspath(os.path.join('..', 'src'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "    \n",
    "import utils\n",
    "import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = 'data/OHSUMED.csv'\n",
    "SVM_MODEL_PATH = 'data/rankSVM.pickle'\n",
    "\n",
    "SVM_LOAD = True\n",
    "\n",
    "TOTAL_SCORE_PATH = 'data/total_score.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 0.7\n",
    "TRAIN_SIZE = 1 - TEST_SIZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = pd.read_csv(DATA_PATH)\n",
    "\n",
    "features_columns = np.array([col for col in documents.columns if col.startswith('feat')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>relevent_val</th>\n",
       "      <th>qid</th>\n",
       "      <th>feat1</th>\n",
       "      <th>feat2</th>\n",
       "      <th>feat3</th>\n",
       "      <th>feat4</th>\n",
       "      <th>feat5</th>\n",
       "      <th>feat6</th>\n",
       "      <th>feat7</th>\n",
       "      <th>feat8</th>\n",
       "      <th>...</th>\n",
       "      <th>feat17</th>\n",
       "      <th>feat18</th>\n",
       "      <th>feat19</th>\n",
       "      <th>feat20</th>\n",
       "      <th>feat21</th>\n",
       "      <th>feat22</th>\n",
       "      <th>feat23</th>\n",
       "      <th>feat24</th>\n",
       "      <th>feat25</th>\n",
       "      <th>doc_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.079442</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.261034</td>\n",
       "      <td>37.330565</td>\n",
       "      <td>11.431241</td>\n",
       "      <td>37.29975</td>\n",
       "      <td>1.138657</td>\n",
       "      <td>...</td>\n",
       "      <td>24.808785</td>\n",
       "      <td>0.393091</td>\n",
       "      <td>57.416517</td>\n",
       "      <td>3.294893</td>\n",
       "      <td>25.0231</td>\n",
       "      <td>3.219799</td>\n",
       "      <td>-3.87098</td>\n",
       "      <td>-3.90273</td>\n",
       "      <td>-3.87512</td>\n",
       "      <td>40626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 28 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   relevent_val  qid  feat1     feat2     feat3     feat4      feat5  \\\n",
       "0           2.0    1    3.0  2.079442  0.272727  0.261034  37.330565   \n",
       "\n",
       "       feat6     feat7     feat8  ...     feat17    feat18     feat19  \\\n",
       "0  11.431241  37.29975  1.138657  ...  24.808785  0.393091  57.416517   \n",
       "\n",
       "     feat20   feat21    feat22   feat23   feat24   feat25  doc_id  \n",
       "0  3.294893  25.0231  3.219799 -3.87098 -3.90273 -3.87512   40626  \n",
       "\n",
       "[1 rows x 28 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_grid = documents.qid.unique()\n",
    "nqid = qid_grid.shape[0]\n",
    "\n",
    "qid_threshold = int(nqid * TRAIN_SIZE)\n",
    "qid_test_threshold = qid_threshold \n",
    "train_qids = qid_grid[:qid_threshold]\n",
    "test_qids = qid_grid[qid_threshold:]\n",
    "\n",
    "assert(train_qids.shape[0] + test_qids.shape[0] == nqid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = np.in1d(documents.qid.values, train_qids)\n",
    "\n",
    "documents_train = documents.loc[train_mask]\n",
    "documents_test = documents.loc[~train_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### pairwise processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = utils.construct_pairwise(documents_train, features_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RankSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rankSVM = None\n",
    "if not SVM_LOAD:\n",
    "    rankSVM = SVC(\n",
    "        kernel='linear',\n",
    "        verbose=100,\n",
    "        C=.1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 6.68 µs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "if not SVM_LOAD:\n",
    "    rankSVM.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not SVM_LOAD:\n",
    "    pickle.dump(rankSVM, open(SVM_MODEL_PATH, 'wb'))\n",
    "    \n",
    "if SVM_LOAD:\n",
    "    with open(SVM_MODEL_PATH, 'rb') as input_stream:\n",
    "        rankSVM = pickle.load(input_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#берем 10% от всех запросов на тест\n",
    "TEST_SIZE = 0.1\n",
    "\n",
    "test_nqid = int(nqid * TEST_SIZE)\n",
    "test_nqid = np.unique(documents_test.qid.values)[-test_nqid:]\n",
    "test_mask = np.in1d(documents_test.qid.values, test_nqid)\n",
    "doc_test = documents_test[test_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cmp(a, b):\n",
    "    x1 = a[features_columns]\n",
    "    x2 = b[features_columns]\n",
    "    x1x2 = (x1-x2).values.reshape(1,-1)\n",
    "    y_pred = rankSVM.predict(x1x2)\n",
    "    if y_pred > 0:\n",
    "        return 1\n",
    "    else:\n",
    "        return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_predict(docs_q):\n",
    "    test_list = [docs_q.iloc[i,:] for i in range(0,len(docs_q))]\n",
    "    return sorted(test_list, key=functools.cmp_to_key(cmp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_forallq = []\n",
    "for q in test_nqid:\n",
    "    docs_q = doc_test[doc_test['qid'] == q]\n",
    "    predict_forallq.append(q_predict(docs_q))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ranked_target_list = []\n",
    "for q_predict in predict_forallq:\n",
    "    ranked_target_list.append(\n",
    "        np.array([x.relevent_val for x in q_predict])\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.ndcg_k(ranked_target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.map_k(ranked_target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics.mrr(ranked_target_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = {\n",
    "    'name': 'RankSVM',\n",
    "    'scores': {\n",
    "        'ndcg_n': metrics.ndcg_k(ranked_target_list),\n",
    "        'map_n': metrics.map_k(ranked_target_list),\n",
    "        'mrr': metrics.mrr(ranked_target_list),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_scores = []\n",
    "if os.path.exists(TOTAL_SCORE_PATH):\n",
    "    with open(TOTAL_SCORE_PATH) as input_stream:\n",
    "        total_scores = json.load(input_stream)\n",
    "    \n",
    "total_scores.append(model_metrics)\n",
    "with open(TOTAL_SCORE_PATH, 'w') as output_stream:\n",
    "    json.dump(total_scores, output_stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
