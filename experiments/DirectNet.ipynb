{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "sys.path.append('..')\n",
    "import tensorflow as tf\n",
    "from src.models.supplementary_code_direct_ranker.DirectRanker import directRanker\n",
    "from src.models.supplementary_code_direct_ranker.helpers import readData, nDCGScorer_cls, MAP_cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "TOTAL_SCORE_PATH = 'data/total_score.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, q_train = readData(data_path=\"OHSUMED_TRAIN.txt\", binary=False, \n",
    "                                     at=10, number_features=25, bin_cutoff=0.9, cut_zeros=False)\n",
    "x_test, y_test, q_test = readData(data_path=\"OHSUMED_TEST.txt\", binary=False, \n",
    "                                  at=10, number_features=25, bin_cutoff=0.9, cut_zeros=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lambda_cost(nn, y0):\n",
    "    return tf.reduce_mean(tf.log(1+tf.exp(nn))-nn)\n",
    "\n",
    "\n",
    "# Load directRanker, train, and test\n",
    "dr = directRanker(\n",
    "    feature_activation=tf.nn.tanh,\n",
    "    ranking_activation=tf.nn.tanh,\n",
    "    # max_steps=10000,\n",
    "    # For debugging\n",
    "    #cost=lambda_cost,\n",
    "    max_steps=12000,\n",
    "    print_step=500,\n",
    "    start_batch_size=4,\n",
    "    end_batch_size=6,\n",
    "    start_qids=20,\n",
    "    end_qids=96,\n",
    "    feature_bias=True,\n",
    "    hidden_layers=[100, 30, 5],\n",
    "    validation_size=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Scale of 0 disables regularizer.\n",
      "step: 0, value: 1.4544665813446045, samples: 4, queries: 20\n",
      "step: 500, value: 0.9079053997993469, samples: 4, queries: 21\n",
      "step: 1000, value: 0.7941244840621948, samples: 4, queries: 22\n",
      "step: 1500, value: 0.8779572248458862, samples: 4, queries: 24\n",
      "step: 2000, value: 0.824640691280365, samples: 4, queries: 25\n",
      "step: 2500, value: 0.7783437967300415, samples: 4, queries: 27\n",
      "step: 3000, value: 0.8378933072090149, samples: 4, queries: 29\n",
      "step: 3500, value: 0.7188060879707336, samples: 4, queries: 31\n",
      "step: 4000, value: 0.7363569140434265, samples: 4, queries: 33\n",
      "step: 4500, value: 0.637319028377533, samples: 4, queries: 36\n",
      "step: 5000, value: 0.596937358379364, samples: 4, queries: 38\n",
      "step: 5500, value: 0.698322057723999, samples: 4, queries: 41\n",
      "step: 6000, value: 0.5675614476203918, samples: 4, queries: 43\n",
      "step: 6500, value: 0.6824402809143066, samples: 4, queries: 46\n",
      "step: 7000, value: 0.5559028387069702, samples: 5, queries: 49\n",
      "step: 7500, value: 0.5277708768844604, samples: 5, queries: 53\n",
      "step: 8000, value: 0.5126115083694458, samples: 5, queries: 56\n",
      "step: 8500, value: 0.5848003029823303, samples: 5, queries: 60\n",
      "step: 9000, value: 0.5166159868240356, samples: 5, queries: 64\n",
      "step: 9500, value: 0.5472977757453918, samples: 5, queries: 69\n",
      "step: 10000, value: 0.4939671754837036, samples: 5, queries: 73\n",
      "step: 10500, value: 0.5024701952934265, samples: 5, queries: 78\n",
      "step: 11000, value: 0.5317769050598145, samples: 5, queries: 84\n",
      "step: 11500, value: 0.5409125685691833, samples: 5, queries: 89\n"
     ]
    }
   ],
   "source": [
    "dr.fit(x_train, y_train, ranking=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = []\n",
    "ideal_rank = []\n",
    "for i in range(len(x_test)):\n",
    "    pred_q = dr.predict_proba(x_test[i])\n",
    "    \n",
    "    sort_idx = np.argsort(np.concatenate(pred_q))\n",
    "    sorted_list = y_test[i][sort_idx][::-1] #по мнению модели\n",
    "    yref = sorted(y_test[i], reverse=True) #идеальное ранжирование для запроса\n",
    "    \n",
    "    prediction.append(sorted_list)\n",
    "    ideal_rank.append(yref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = [x.flatten() for x in prediction]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = lambda x: 2 ** x - 1\n",
    "d = lambda i: 1 / np.log2(i + 1)\n",
    "def _dcg_k(ranked_target, k=None):\n",
    "    if k is None:\n",
    "        k = ranked_target.shape[0]\n",
    "\n",
    "    ranked_target = ranked_target[:k]\n",
    "    return np.sum(\n",
    "        g(ranked_target) * d(np.arange(1, k + 1))\n",
    "    )\n",
    "\n",
    "\n",
    "def _ndcg_k(ranked_target, k=None):\n",
    "    dcg_value = _dcg_k(ranked_target, k)\n",
    "\n",
    "    ideal_dcg = _dcg_k(\n",
    "        np.sort(ranked_target)[::-1],\n",
    "        k\n",
    "    )\n",
    "\n",
    "    if ideal_dcg == 0:\n",
    "        return 1\n",
    "    return dcg_value / ideal_dcg\n",
    "\n",
    "\n",
    "def dcg_k(ranked_target_list, k=None):\n",
    "    scores = []\n",
    "    for ranked_target in ranked_target_list:\n",
    "\n",
    "        scores.append(\n",
    "            _dcg_k(ranked_target, k)\n",
    "        )\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def ndcg_k(ranked_target_list, k=None):\n",
    "    scores = []\n",
    "    for ranked_target in ranked_target_list:\n",
    "\n",
    "        scores.append(\n",
    "            _ndcg_k(ranked_target, k)\n",
    "        )\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def _precision_at_k(ranked_target):\n",
    "    return np.cumsum(ranked_target / 2) / np.arange(1, ranked_target.shape[0] + 1)\n",
    "\n",
    "\n",
    "def _aprecision_at_k(ranked_target, k=None):\n",
    "    if k is None:\n",
    "        k = ranked_target.shape[0]\n",
    "\n",
    "    precisions = _precision_at_k(ranked_target)[:k]\n",
    "    ranked_target = ranked_target[:k]\n",
    "\n",
    "    result = ranked_target * 1. / ranked_target.sum()\n",
    "\n",
    "    return np.sum(result * precisions)\n",
    "\n",
    "\n",
    "def map_k(ranked_target_list, k=None):\n",
    "    scores = []\n",
    "    for ranked_target in ranked_target_list:\n",
    "\n",
    "        scores.append(\n",
    "            _aprecision_at_k(ranked_target, k)\n",
    "        )\n",
    "\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "def mrr(ranked_target_list):\n",
    "    total_score = 0\n",
    "    for ranked_target in ranked_target_list:\n",
    "        score = ranked_target.argmax()\n",
    "        if ranked_target.max() != 2.:\n",
    "            score = ranked_target.shape[0]\n",
    "\n",
    "        total_score += score\n",
    "\n",
    "    return total_score / len(ranked_target_list)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5846116029437558"
      ]
     },
     "execution_count": 180,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ndcg_k(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24437291502214054"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "map_k(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.1"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_metrics = {\n",
    "    'name': 'DirectRanker',\n",
    "    'scores': {\n",
    "        'ndcg_n': ndcg_k(prediction),\n",
    "        'map_n': map_k(prediction),\n",
    "        'mrr': mrr(prediction),\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "total_scores = []\n",
    "if os.path.exists(TOTAL_SCORE_PATH):\n",
    "    with open(TOTAL_SCORE_PATH) as input_stream:\n",
    "        total_scores = json.load(input_stream)\n",
    "    \n",
    "total_scores.append(model_metrics)\n",
    "with open(TOTAL_SCORE_PATH, 'w') as output_stream:\n",
    "    json.dump(total_scores, output_stream)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
